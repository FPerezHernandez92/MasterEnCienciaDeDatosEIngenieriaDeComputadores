mean(qda.pred$class==Smarket.2005$Direction)
#Y ahora obtenemos con QDA un 59,9% sobre el test, mejorando a LDA
partimat(Direction~Lag1+Lag2, data=Smarket ,method="qda")
#Pero se ve que aún es bastante dificil
## LDA
iris.lda<-lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length
+ Petal.Width,  data = iris)
iris.lda
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length +
Petal.Width, data=iris, method="lda")
#Vemos como hemos usado LDA sobre el conjunto de datos IRIS
data(iris)
TrainData <- iris[,1:4]
TrainClasses <- iris[,5]
ldaFit <- train(TrainData, TrainClasses,
method = "lda",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
confusionMatrix(ldaFit)
#Obtenemos un accuracy del 98% lo que es muy bueno el método LDA
qdaFit <- train(TrainData, TrainClasses,
method = "qda",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
confusionMatrix(qdaFit)
#Con QDA obtenemos un 97% el cual no está nada mal pero sigue siendo mejor LDA
#EJERCICIO 1:
set.seed(1234)
# - Try LDA with all Lag variables
Smarket
trainNumber <- cbind(c(1:1000))
trainData = Smarket[trainNumber,-9]
trainDataLabel = as.factor(Smarket[trainNumber,9])
testData = Smarket[-trainNumber,-9]
testDataLabel = as.factor(Smarket[-trainNumber,9])
lda.fit=lda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5
,data=Smarket,train=trainData)
lda.fit
lda.pred=predict(lda.fit,testData)
table(lda.pred$class,testDataLabel)
#Aquí podemos ver la tabla comparativa del test
mean(lda.pred$class==testDataLabel)
confusionMatrix(data=lda.pred$class,reference = testDataLabel)
#Por lo que tenemos un accuracy del 56,4% el cual no es demasiado bueno
#Con caret:
ldaFit <- train(form=Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket,
subset = trainNumber, method="lda",
metric="Accuracy",
preProcess = c("center","scale"),
tuneLength = 10,
trControl=trainControl(method ="cv",number=10))
summary(ldaFit)
ldaPredict <- predict(ldaFit, newdata = testData)
confusionMatrix(data = ldaPredict, reference = testDataLabel)
#Obtenemos un 57,6% por lo que ha mejorado un poco
matrizAccuracy <- matrix(c(57.6))
colnames(matrizAccuracy) <- c("LDA")
# - Make a quick comparison between logistic regression and LDA
glmFit <- train(form=Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket,
subset = trainNumber, method="glm",
metric="Accuracy",
preProcess = c("center","scale"),
tuneLength = 10,
trControl=trainControl(method ="cv",number=10))
summary(glmFit)
glmPredict <- predict(glmFit, newdata = testData)
confusionMatrix(data = glmPredict, reference = testDataLabel)
#Con este método obtenemos un 57,6% igual que LDA con caret
matrizAccuracyaux <- matrix(c(57.6))
colnames(matrizAccuracyaux) <- c("GLM")
matrizAccuracy <- cbind(matrizAccuracy,matrizAccuracyaux)
# - Try with QDA and compare all three methods. Plot the results
qdaFit <- train(form=Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket,
subset = trainNumber, method="qda",
metric="Accuracy",
preProcess = c("center","scale"),
tuneLength = 10,
trControl=trainControl(method ="cv",number=10))
qdaPredict <- predict(qdaFit, newdata = testData)
confusionMatrix(data = qdaPredict, reference = testDataLabel)
#Obtenemos un 54,8% el cual es más malo que LDA y QDA
matrizAccuracyaux <- matrix(c(54.8))
colnames(matrizAccuracyaux) <- c("QDA")
matrizAccuracy <- cbind(matrizAccuracy,matrizAccuracyaux)
#Para visualizar los resultados me he decidido por mostrar los accuracy de los distintos métodos:
barplot(matrizAccuracy,ylim=c(0,100),col="green",main="Accuracy distintos métodos",ylab="Accuracy %")
set.seed(1234)
#EJERCICIO 2: Using only the information in file clasif_train_alumnos.csv:
train_alumnos <- read.csv("ICD/clasif_train_alumnos.csv",stringsAsFactors = FALSE)
train_alumnos <- read.csv("clasif_train_alumnos.csv",stringsAsFactors = FALSE)
# - Compare lda and qda using Wilcoxon
help("wilcox.test")
wilcox.test(train_alumnos$out_train_lda, train_alumnos$out_train_qda, paired=TRUE)
#Tenemos un p-valor mayor que 0.05 ya que da 0.17, por lo que no tienen diferencias entre si
# - Perform a multiple comparison using Friedman
help("friedman.test")
dim(train_alumnos)
head(train_alumnos)
m<-matrix(0,20,3)
m[,1] <- train_alumnos$out_train_knn
m[,2] <- train_alumnos$out_train_lda
m[,3] <- train_alumnos$out_train_qda
friedman.test(m)
#Tenemos un p-valor mayor que 0.05 ya que da 0.52, por lo que no tienen diferencias entre si
# - Using Holm see if there is a winning algorithm (even if Friedman says ther is no chance...)
help("pairwise.wilcox.test")
tam <- dim(m)
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(m,groups,p.adjust="holm",paired=TRUE)
#Al no salir ningún p-valor menor que 0.05 no hay ningún resultado significativo
set.seed(1234)
########################
# EJEMPLO TRANSPARENCIAS
########################
# import the CSV file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# examine the structure of the wbcd data frame
str(wbcd)
# drop the id feature
wbcd <- wbcd[,-1]
# table of diagnosis
table(wbcd$diagnosis)
# recode diagnosis as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
label = c("Benign", "Malignant"))
# table or proportions with more informative labels
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
# summarize three numeric features
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
View(wbcd)
# create normalization function
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# test normalization function - result should be identical
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
# normalize the wbcd data
wbcd_n <- as.data.frame(lapply(wbcd[,2:31], normalize))
# confirm that normalization worked
summary(wbcd_n$area_mean)
# Visualize data
plot(wbcd[,2:5])
plot(wbcd_n[,1:4], col=wbcd[,1])
# Calculate correlation
cor(wbcd[,2:5])
# create training and test data
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# create labels for training and test data
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
## Training a model on the data ----
# load the "class" library
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
## Evaluating model performance ----
table(wbcd_test_pred,wbcd_test_labels)
## Using caret
require(caret)
knnModel <- train(x = wbcd[1:469,-1], y = wbcd[1:469,1], method = "knn", preProc = c("center","scale"))
knnModel
knnFit <- train(wbcd_train, wbcd_train_labels, method="knn", metric="Accuracy", tuneGrid = data.frame(.k=1:15))
knnFit
knnPred <- predict(knnModel, newdata = wbcd_test)
testData = wbcd[470:569,1]
postResample(pred = knnPred, obs = testData)
#Por lo que ya se ha podido ver el funcionamiento y el resultado proporcionado
set.seed(123456)
#EJERCICIO 1: Try with different k choices and do a quick comparison. You can draw a plot to show the results.
#Creo una función con la que calcular un modelo Knn con distintos valores de K y que guarde el valor accuracy
aplicar_knn <- function(k){
mi_knn <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k)
accu <- postResample(pred = mi_knn, obs = testData)[1]
}
#Aplico la funcíon para obtener los accuracy
accuracy <- sapply(1:400,aplicar_knn)
#Primero habia puesto 10 pero al ver que no habia demasiada diferencia he usado 400
accuracy
#Pinto las gráficas para ver los valores de accuracy
plot(x=1:400,y=accuracy)
plot(x=1:100,y=accuracy[1:100],type="l")
plot(x=1:40, y=accuracy[1:40],type="l")
max(accuracy)
plot(x=1:25,y=accuracy[1:25],type="p")
lines(x=1:25,y=accuracy[1:25],type="l",col="blue")
#Vemos como hay 5 valores con accuracy = 0.98 asique tenemos distintos k buenos
######################
#SEGUNDA PARTE DEL PDF
######################
set.seed(1234)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
# Let's see the data...
cor(Smarket) # This won't work, why)
#Porque debe ser variables numéricas
cor(Smarket[,-9]) # Note that Volume has some correlation with Year...
boxplot(Smarket$Volume~Smarket$Year)
# Direction is derive from Today
cor(as.numeric(Smarket$Direction),Smarket$Today)
# Logistic regression
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs <- predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred <- ifelse(glm.probs>0.5,"Up","Down")
attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
# Make training and test set
train <- Year<2005
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs <- predict(glm.fit,newdata=Smarket[!
train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
#Overfitting!
#Fit smaller model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!
train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
glmFit <- train(train_x, y = train_y,
method = "glm",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
train_x <- Year<2005
test_x <- [!train_x]
test_x <- [!train_x,]
test_x <- [,!train_x]
test_x <- Smarket[!train_x,]
glmFit <- train(train_x, y = train_y,
method = "glm",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
train_y <- Smarket[!train_x,]
glmFit <- train(train_x, y = train_y,
method = "glm",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
set.seed(1234)
#Divido en train y test
total_train <- length(Smarket$Year)*0.8
set.seed(1234)
#Divido en train y test
total_train <- length(Smarket$Year)*0.8
Smarket_train <- Smarket[1:total_train,-9]
#He quitado la última ya que es la que vamos a predecir
Smarket_train_labels <- as.factor(Smarket[1:total_train,]$Direction)
Smarket_test <- Smarket[(total_train+1):length(Smarket$Year),-9]
Smarket_test_labels <- as.factor(Smarket[(total_train+1):length(Smarket$Year),]$Direction)
glmFit <- train(Smarket_train, Smarket_train_labels, method="glm",
metric="Accuracy",
trControl=trainControl(method ="cv",number=10))
warnings()
#glm.fit: el alogritmo no converge por lo que hay que darle mas iteraciones
glmFit <- train(Smarket_train, Smarket_train_labels, method="glm",
metric="Accuracy",
trControl=trainControl(method ="cv",number=10),
control=glm.control(maxit=40))
#He probado de 10 en 10 hasta que con 40 ha funcionado
warnings()
#glm.fit: se suponen que son problemas de preciosión pero no he conseguido arreglarlos
glmFit
#Tenemos un Accuracy del 99,6% por lo que el modelo es muy bueno, o está muy sobreajustado
#Probamos en test:
glmPredict <- predict(glmFit, newdata = Smarket_test)
confusionMatrix(data = glmPredict, reference = Smarket_test_labels)
train_x <- Year<2005
train_y <- as.factor(Smarket[Year<2005])
train_y <- as.factor(Smarket[Year<2005,])
train_y <- as.factor(Smarket[Year<2005,]$Direction)
glmFit <- train(train_x, y = train_y,
method = "glm",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
warnings()
total_train <- length(Smarket$Year)*0.8
total_train <- length(Smarket$Year)*0.8
train_x <- Smarket[1:total_train,-9]
#He quitado la última ya que es la que vamos a predecir
train_y <- as.factor(Smarket[1:total_train,]$Direction)
glmFit <- train(train_x, y = train_y,
method = "glm",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
warnings()
set.seed(1234)
library(MASS)
library(ISLR)
library(klaR)
## Linear Discriminant Analysis
lda.fit=lda(Direction~Lag1+Lag2
,data=Smarket,
subset=Year<2005)
lda.fit
plot(lda.fit, type="both")
Smarket.2005=subset(Smarket,Year==2005)
lda.pred=predict(lda.fit,Smarket.2005)
lda.pred$class[1:5] ### arreglado
class(lda.pred)
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
#Aquí podemos ver la tabla comparativa del test
mean(lda.pred$class==Smarket.2005$Direction)
#Una media del 55,95% que no está mal para comenzar
partimat(Direction~Lag1+Lag2, data=Smarket ,method="lda")
#Vemos como es dificil obtener un modelo que se ajuste correctamente
## QDA
qda.fit=qda(Direction~Lag1+Lag2, data=Smarket, subset=Year<2005)
qda.fit
qda.fit$scaling
plot(qda.fit, type="both")##omitir
Smarket.2005=subset(Smarket,Year==2005)
qda.pred=predict(qda.fit,Smarket.2005)
class(qda.pred)
data.frame(qda.pred)[1:5,]
table(qda.pred$class,Smarket.2005$Direction)
#Vemos de nuevo la tabla comparativa del test
mean(qda.pred$class==Smarket.2005$Direction)
#Y ahora obtenemos con QDA un 59,9% sobre el test, mejorando a LDA
partimat(Direction~Lag1+Lag2, data=Smarket ,method="qda")
#Pero se ve que aún es bastante dificil
## LDA
iris.lda<-lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length
+ Petal.Width,  data = iris)
iris.lda
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length +
Petal.Width, data=iris, method="lda")
#Vemos como hemos usado LDA sobre el conjunto de datos IRIS
data(iris)
TrainData <- iris[,1:4]
TrainClasses <- iris[,5]
ldaFit <- train(TrainData, TrainClasses,
method = "lda",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
confusionMatrix(ldaFit)
#Obtenemos un accuracy del 98% lo que es muy bueno el método LDA
qdaFit <- train(TrainData, TrainClasses,
method = "qda",
preProcess = c("center", "scale"),
tuneLength = 10,
trControl = trainControl(method = "cv"))
confusionMatrix(qdaFit)
#Con QDA obtenemos un 97% el cual no está nada mal pero sigue siendo mejor LDA
#EJERCICIO 1:
set.seed(1234)
# - Try LDA with all Lag variables
Smarket
trainNumber <- cbind(c(1:1000))
trainData = Smarket[trainNumber,-9]
trainDataLabel = as.factor(Smarket[trainNumber,9])
testData = Smarket[-trainNumber,-9]
testDataLabel = as.factor(Smarket[-trainNumber,9])
lda.fit=lda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5
,data=Smarket,train=trainData)
lda.fit
lda.pred=predict(lda.fit,testData)
table(lda.pred$class,testDataLabel)
#Aquí podemos ver la tabla comparativa del test
mean(lda.pred$class==testDataLabel)
confusionMatrix(data=lda.pred$class,reference = testDataLabel)
#Por lo que tenemos un accuracy del 56,4% el cual no es demasiado bueno
#Con caret:
ldaFit <- train(form=Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket,
subset = trainNumber, method="lda",
metric="Accuracy",
preProcess = c("center","scale"),
tuneLength = 10,
trControl=trainControl(method ="cv",number=10))
summary(ldaFit)
ldaPredict <- predict(ldaFit, newdata = testData)
confusionMatrix(data = ldaPredict, reference = testDataLabel)
#Obtenemos un 57,6% por lo que ha mejorado un poco
matrizAccuracy <- matrix(c(57.6))
colnames(matrizAccuracy) <- c("LDA")
# - Make a quick comparison between logistic regression and LDA
glmFit <- train(form=Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket,
subset = trainNumber, method="glm",
metric="Accuracy",
preProcess = c("center","scale"),
tuneLength = 10,
trControl=trainControl(method ="cv",number=10))
summary(glmFit)
glmPredict <- predict(glmFit, newdata = testData)
confusionMatrix(data = glmPredict, reference = testDataLabel)
#Con este método obtenemos un 57,6% igual que LDA con caret
matrizAccuracyaux <- matrix(c(57.6))
colnames(matrizAccuracyaux) <- c("GLM")
matrizAccuracy <- cbind(matrizAccuracy,matrizAccuracyaux)
# - Try with QDA and compare all three methods. Plot the results
qdaFit <- train(form=Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket,
subset = trainNumber, method="qda",
metric="Accuracy",
preProcess = c("center","scale"),
tuneLength = 10,
trControl=trainControl(method ="cv",number=10))
qdaPredict <- predict(qdaFit, newdata = testData)
confusionMatrix(data = qdaPredict, reference = testDataLabel)
#Obtenemos un 54,8% el cual es más malo que LDA y QDA
matrizAccuracyaux <- matrix(c(54.8))
colnames(matrizAccuracyaux) <- c("QDA")
matrizAccuracy <- cbind(matrizAccuracy,matrizAccuracyaux)
#Para visualizar los resultados me he decidido por mostrar los accuracy de los distintos métodos:
barplot(matrizAccuracy,ylim=c(0,100),col="green",main="Accuracy distintos métodos",ylab="Accuracy %")
set.seed(1234)
#EJERCICIO 2: Using only the information in file clasif_train_alumnos.csv:
train_alumnos <- read.csv("clasif_train_alumnos.csv",stringsAsFactors = FALSE)
# - Compare lda and qda using Wilcoxon
help("wilcox.test")
wilcox.test(train_alumnos$out_train_lda, train_alumnos$out_train_qda, paired=TRUE)
#Tenemos un p-valor mayor que 0.05 ya que da 0.17, por lo que no tienen diferencias entre si
# - Perform a multiple comparison using Friedman
help("friedman.test")
dim(train_alumnos)
head(train_alumnos)
m<-matrix(0,20,3)
m[,1] <- train_alumnos$out_train_knn
m[,2] <- train_alumnos$out_train_lda
m[,3] <- train_alumnos$out_train_qda
friedman.test(m)
#Tenemos un p-valor mayor que 0.05 ya que da 0.52, por lo que no tienen diferencias entre si
# - Using Holm see if there is a winning algorithm (even if Friedman says ther is no chance...)
help("pairwise.wilcox.test")
tam <- dim(m)
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(m,groups,p.adjust="holm",paired=TRUE)
#Al no salir ningún p-valor menor que 0.05 no hay ningún resultado significativo
rm(list=ls())
setwd("~/Dropbox/zMaster/zRStudio/Master-en-Ciencia-De-Datos-e-Ingeniería-de-Computadores/1 Introduccion a la Ciencia de Datos/5 Trabajo Final")
australian <- read.csv("australian/australian.dat", comment.char = "@", header = FALSE)
is.data.frame(australian)
is.data.frame(australian)
#Podemos ver como es un data frame la estructura de datos
ncol(australian)
nrow(australian)
#Con 15 columnas y 690 filas
#Tenemos que las variables 2,3 y 7 son numéricas, y el resto son int
#APARTADO A1: Media, desviación estandar...
summary(australian)
#Obtenemos las medias de las variables o con summary o con apply
medias_australian <- apply(australian,2,mean)
medias_australian
#Para la desviación estandar usamos apply
desviacion_estandar_australian <- apply(australian,2,sd)
desviacion_estandar_australian
#Varianza
varianza_australian <- apply(australian,2,var)
varianza_australian
#Desviación absoluta de la mediana
desviacion_absoluta_de_la_mediana_australian <- apply(australian,2,mad)
desviacion_absoluta_de_la_mediana_australian
#Rango intercuartil
rango_intercuartil_australian <- apply(australian,2,IQR)
rango_intercuartil_australian
#APARTADO A2: Gráficos
plot(australian)
#Directamente con plot, podemos ver las graficas de cada variable con el resto
par(mfrow=c(3,3))
nombres_australian = names(australian)
plot_con_respecto_a_y <- function(x,y){
plot(australian[,y]~australian[,x],xlab=nombres_australian[x],ylab="15")
}
sapply(1:9,plot_con_respecto_a_y,15)
par(mfrow=c(3,2))
sapply(10:14,plot_con_respecto_a_y,15)
plot(australian)
correlacion_australian <- cor(australian)
correlacion_australian <- cor(australian)[,15]
correlacion_australian[cor(australian)[,15] > 0.5 | cor(australian)[,15] < -0.5]
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
detach()
rm(list=ls())
australian <- read.csv("australian/australian.dat", comment.char = "@", header = FALSE)
setwd("~/Dropbox/zMaster/zRStudio/Master-en-Ciencia-De-Datos-e-Ingeniería-de-Computadores/1 Introduccion a la Ciencia de Datos/5 Trabajo Final")
